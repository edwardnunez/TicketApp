name: Performance Tests

# Ejecutar en PRs a main y en schedule
on:
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'backend/**'
      - 'docker-compose.yml'
  schedule:
    # Todos los d√≠as a las 2 AM
    - cron: '0 2 * * *'
  workflow_dispatch:
    # Permite ejecuci√≥n manual

jobs:
  performance-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      # MongoDB para tests
      mongodb:
        image: mongo:latest
        ports:
          - 27017:27017
        env:
          MONGO_INITDB_ROOT_USERNAME: root
          MONGO_INITDB_ROOT_PASSWORD: example

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '18'
          cache: 'npm'

      - name: Setup Java (for JMeter)
        uses: actions/setup-java@v3
        with:
          distribution: 'temurin'
          java-version: '11'

      - name: Install JMeter
        run: |
          wget -q https://archive.apache.org/dist/jmeter/binaries/apache-jmeter-5.6.2.tgz
          tar -xzf apache-jmeter-5.6.2.tgz
          echo "$(pwd)/apache-jmeter-5.6.2/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: |
          cd backend/gatewayservice && npm ci
          cd ../userservice && npm ci
          cd ../eventservice && npm ci
          cd ../ticketservice && npm ci
          cd ../locationservice && npm ci

      - name: Create .env file
        run: |
          cat > backend/.env << EOF
          JWT_SECRET=${{ secrets.JWT_SECRET_TEST }}
          MONGODB_URI=mongodb://root:example@localhost:27017/
          USER_SERVICE_URL=http://localhost:8001
          EVENT_SERVICE_URL=http://localhost:8003
          TICKET_SERVICE_URL=http://localhost:8002
          LOCATION_SERVICE_URL=http://localhost:8004
          EOF

      - name: Start services with Docker Compose
        run: |
          docker-compose up -d
          sleep 30

      - name: Wait for services to be ready
        run: |
          echo "Waiting for Gateway..."
          timeout 60 bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'
          echo "Gateway is ready!"

      - name: Verify services
        run: |
          echo "Checking all services..."
          curl http://localhost:8000/health
          curl http://localhost:8000/events
          echo "All services are responding!"

      - name: Run Performance Tests
        id: jmeter
        run: |
          cd jmeter
          jmeter -n \
            -t TicketApp-Performance-Test.jmx \
            -l results/ci-test-${{ github.run_number }}.jtl \
            -e -o reports/ci-test-${{ github.run_number }}/ \
            -JGATEWAY_HOST=localhost \
            -JGATEWAY_PORT=8000

      - name: Check performance thresholds
        run: |
          # Analizar resultados
          python3 << 'EOF'
          import csv
          import sys

          with open('jmeter/results/ci-test-${{ github.run_number }}.jtl', 'r') as f:
              reader = csv.DictReader(f)

              total = 0
              errors = 0
              times = []

              for row in reader:
                  total += 1
                  if row['success'] == 'false':
                      errors += 1
                  times.append(int(row['elapsed']))

              error_rate = (errors / total) * 100 if total > 0 else 0
              avg_time = sum(times) / len(times) if times else 0
              max_time = max(times) if times else 0

              print(f"Total Requests: {total}")
              print(f"Failed Requests: {errors}")
              print(f"Error Rate: {error_rate:.2f}%")
              print(f"Average Response Time: {avg_time:.0f}ms")
              print(f"Max Response Time: {max_time}ms")

              # Thresholds
              if error_rate > 5.0:
                  print("‚ùå FAIL: Error rate too high!")
                  sys.exit(1)

              if avg_time > 1000:
                  print("‚ùå FAIL: Average response time too high!")
                  sys.exit(1)

              print("‚úÖ PASS: All performance thresholds met!")
          EOF

      - name: Upload JMeter Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: jmeter-results-${{ github.run_number }}
          path: |
            jmeter/results/
            jmeter/reports/
          retention-days: 30

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const path = 'jmeter/reports/ci-test-${{ github.run_number }}/statistics.json';

            if (fs.existsSync(path)) {
              const stats = JSON.parse(fs.readFileSync(path, 'utf8'));

              let comment = '## üìä Performance Test Results\n\n';
              comment += '| Metric | Value |\n';
              comment += '|--------|-------|\n';
              comment += `| Total Samples | ${stats.Total.sampleCount} |\n`;
              comment += `| Error % | ${stats.Total.errorPct}% |\n`;
              comment += `| Average RT | ${stats.Total.meanResTime}ms |\n`;
              comment += `| 90th Percentile | ${stats.Total.pct1ResTime}ms |\n`;
              comment += `| 95th Percentile | ${stats.Total.pct2ResTime}ms |\n`;
              comment += `| Throughput | ${stats.Total.throughput}/sec |\n`;
              comment += '\n';
              comment += `[View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n`;

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

      - name: Generate performance trend
        if: github.ref == 'refs/heads/main'
        run: |
          # Guardar m√©tricas para comparaci√≥n hist√≥rica
          python3 << 'EOF'
          import json
          import os
          from datetime import datetime

          stats_file = 'jmeter/reports/ci-test-${{ github.run_number }}/statistics.json'

          if os.path.exists(stats_file):
              with open(stats_file, 'r') as f:
                  stats = json.load(f)

              trend = {
                  'timestamp': datetime.now().isoformat(),
                  'commit': '${{ github.sha }}',
                  'run_number': '${{ github.run_number }}',
                  'error_rate': stats['Total']['errorPct'],
                  'avg_response_time': stats['Total']['meanResTime'],
                  'p95': stats['Total']['pct2ResTime'],
                  'throughput': stats['Total']['throughput']
              }

              # Append to trend file
              trend_file = 'performance-trends.jsonl'
              with open(trend_file, 'a') as f:
                  f.write(json.dumps(trend) + '\n')

              print(f"Performance trend saved: {trend}")
          EOF

      - name: Check for performance regression
        if: github.event_name == 'pull_request'
        run: |
          # Comparar con baseline
          python3 << 'EOF'
          import json
          import sys

          # Leer resultados actuales
          with open('jmeter/reports/ci-test-${{ github.run_number }}/statistics.json', 'r') as f:
              current = json.load(f)

          # Leer baseline (√∫ltima prueba en main)
          try:
              with open('performance-baseline.json', 'r') as f:
                  baseline = json.load(f)
          except FileNotFoundError:
              print("No baseline found, skipping regression check")
              sys.exit(0)

          current_rt = current['Total']['meanResTime']
          baseline_rt = baseline['Total']['meanResTime']

          regression = ((current_rt - baseline_rt) / baseline_rt) * 100

          print(f"Current RT: {current_rt}ms")
          print(f"Baseline RT: {baseline_rt}ms")
          print(f"Regression: {regression:+.2f}%")

          if regression > 20:  # 20% slower
              print("‚ùå FAIL: Performance regression detected!")
              sys.exit(1)

          print("‚úÖ PASS: No significant performance regression")
          EOF

      - name: Cleanup
        if: always()
        run: |
          docker-compose down -v
          docker system prune -f

  # Job para comparar con versi√≥n anterior
  compare-performance:
    runs-on: ubuntu-latest
    needs: performance-test
    if: github.event_name == 'pull_request'

    steps:
      - name: Download current results
        uses: actions/download-artifact@v3
        with:
          name: jmeter-results-${{ github.run_number }}
          path: current/

      - name: Compare with previous
        run: |
          echo "Comparing performance with previous version..."
          # Aqu√≠ puedes agregar l√≥gica de comparaci√≥n m√°s sofisticada

  # Job para alertas
  alert-on-failure:
    runs-on: ubuntu-latest
    needs: performance-test
    if: failure() && github.ref == 'refs/heads/main'

    steps:
      - name: Send Slack notification
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: |
            ‚ùå Performance tests failed on main branch!
            Commit: ${{ github.sha }}
            Author: ${{ github.actor }}
            View results: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        if: always()
